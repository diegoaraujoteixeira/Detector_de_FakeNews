# -*- coding: utf-8 -*-
"""model_trainer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ezceli09IdtV9t0f5AfnRPT6YhAaxUn1

###Carregamento e Gerenciamento do Dataset de Notícias Falsas e Reais
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd # Precisaremos do pandas para ler os CSVs
import os
import kagglehub # Para baixar o dataset

from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D, Dense, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns # Para visualização da matriz de confusão

print("Versão do TensorFlow:", tf.__version__)
print("Versão do Pandas:", pd.__version__)
print("Baixando o dataset 'fake-and-real-news-dataset' do KaggleHub...")
try:
    path = kagglehub.dataset_download("clmentbisaillon/fake-and-real-news-dataset")
    print("Path to dataset files:", path)

    # Carregar os dois arquivos CSV
    fake_news_df = pd.read_csv(os.path.join(path, 'Fake.csv'))
    true_news_df = pd.read_csv(os.path.join(path, 'True.csv'))

    print("\nDataFrame de Notícias Falsas (primeiras 5 linhas):")
    display(fake_news_df.head())
    print("\nDataFrame de Notícias Reais (primeiras 5 linhas):")
    display(true_news_df.head())

    # Adicionar a coluna de rótulo (target)
    fake_news_df['label'] = 0  # 0 para Notícias Falsas
    true_news_df['label'] = 1  # 1 para Notícias Reais

    # Combinar os dois DataFrames
    # Vamos usar as colunas 'title' e 'text' para o conteúdo da notícia.
    # O dataset original tem 'date', que não é relevante para a classificação do texto.
    df = pd.concat([fake_news_df[['title', 'text', 'label']],
                    true_news_df[['title', 'text', 'label']]],
                   ignore_index=True)

    # Criar uma única coluna de texto combinando título e corpo da notícia
    # É uma prática comum, pois o título costuma ter informações importantes
    df['full_text'] = df['title'] + " " + df['text']

    # Verificar valores ausentes na coluna 'full_text'
    print("\nValores ausentes em 'full_text' antes do tratamento:")
    print(df['full_text'].isnull().sum())

    # Preencher valores ausentes com uma string vazia para evitar erros na vetorização
    df['full_text'] = df['full_text'].fillna('')

    print("\nValores ausentes em 'full_text' após o tratamento:")
    print(df['full_text'].isnull().sum())

    # Embaralhar o DataFrame combinado
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)

    print(f"\nTotal de notícias carregadas: {len(df)}")
    print("Distribuição dos rótulos:")
    print(df['label'].value_counts())
    print("\nProporção dos rótulos:")
    print(df['label'].value_counts(normalize=True))

except Exception as e:
    print(f"Erro ao carregar o dataset: {e}")
    print("Verifique se você tem acesso ao KaggleHub ou se os arquivos CSV estão na estrutura esperada.")
    exit() # Interrompe a execução se não conseguir carregar os dados

# Separar features (X) e variável alvo (y)
all_texts = df['full_text'].tolist()
all_labels = df['label'].values

# Divisão dos Dados em Treino, Validação e Teste
# 80% treino + validação (do total), 20% teste
X_train_val, X_test, y_train_val, y_test = train_test_split(
    all_texts, np.array(all_labels), test_size=0.2, random_state=42, stratify=all_labels
)

# 80% treino, 20% validação (do conjunto treino+validação)
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val
) # 0.25 * 0.8 = 0.2, então 60% treino, 20% validação, 20% teste

print(f"\nNúmero de notícias de treino: {len(X_train)}")
print(f"Número de notícias de validação: {len(X_val)}")
print(f"Número de notícias de teste: {len(X_test)}")

# Converter para tf.data.Dataset para otimização do pipeline
BUFFER_SIZE = 10000
BATCH_SIZE = 32

train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(BUFFER_SIZE)
val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(BUFFER_SIZE)
test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE).prefetch(BUFFER_SIZE)

print("Distribuição de labels em y_train:")
print(pd.Series(y_train).value_counts(normalize=True))
print("\nDistribuição de labels em y_val:")
print(pd.Series(y_val).value_counts(normalize=True))
print("\nDistribuição de labels em y_test:")
print(pd.Series(y_test).value_counts(normalize=True))

"""###2. Pré-processamento e Vetorização do Texto"""

# Configurações para a camada TextVectorization
VOCAB_SIZE = 20000 # Aumentei o vocabulário, pois notícias tendem a ter mais palavras únicas
SEQUENCE_LENGTH = 512 # Aumentei o comprimento da sequência para acomodar notícias mais longas

# Cria a camada de vetorização de texto
text_vectorizer = TextVectorization(
    max_tokens=VOCAB_SIZE,
    output_mode='int',
    output_sequence_length=SEQUENCE_LENGTH,
    standardize="strip_punctuation",
    split="whitespace"
)

# Adapta a camada de vetorização aos dados de treinamento (apenas os textos)
text_for_adaptation = tf.data.Dataset.from_tensor_slices(X_train)
text_vectorizer.adapt(text_for_adaptation)

# Visualizar o vocabulário construído
print(f"\nTamanho do vocabulário gerado: {len(text_vectorizer.get_vocabulary())}")
print("Primeiras 10 palavras do vocabulário:", text_vectorizer.get_vocabulary()[:10])

# Função para aplicar a vetorização aos datasets
def vectorize_text_and_label(text, label):
    text = tf.expand_dims(text, -1)
    return text_vectorizer(text), label

# Aplica a vetorização aos datasets de treino, validação e teste
train_ds_vectorized = train_ds.map(vectorize_text_and_label).cache().prefetch(BUFFER_SIZE)
val_ds_vectorized = val_ds.map(vectorize_text_and_label).cache().prefetch(BUFFER_SIZE)
test_ds_vectorized = test_ds.map(vectorize_text_and_label).cache().prefetch(BUFFER_SIZE)

# Verificar o formato dos dados após a vetorização
print("\nVerificando formato dos dados vetorizados:")
for text_batch, label_batch in train_ds_vectorized.take(1):
    print(f"Shape do lote de texto vetorizado: {text_batch.shape}")
    print(f"Shape do lote de labels: {label_batch.shape}")
    print(f"Exemplo de sequência vetorizada (primeira notícia):\n {text_batch[0].numpy()}")

"""###3. Construção do Modelo Deep Learning

"""

EMBEDDING_DIM = 300 # Aumentei a dimensão dos embeddings para capturar mais complexidade. Ajuste este valor!

model = Sequential([
    # Camada de Embedding: Converte IDs de palavras em vetores densos
    Embedding(VOCAB_SIZE + 1, EMBEDDING_DIM, input_length=SEQUENCE_LENGTH),

    # GlobalAveragePooling1D: Reduz a sequência de vetores para um único vetor por notícia
    GlobalAveragePooling1D(),

    # Camadas Densas (Fully Connected)
    Dense(512), # Mais neurônios para um dataset maior/mais complexo
    BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    Dropout(0.5), # Aumentei o dropout para combater overfitting com um modelo maior

    Dense(128),
    BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    Dropout(0.5),

    Dense(64),
    BatchNormalization(),
    tf.keras.layers.Activation('relu'),
    Dropout(0.4),

    # Camada de Saída: 1 neurônio com ativação sigmoid para classificação binária (Falsa/Verdadeira)
    Dense(1, activation='sigmoid')
])

# Compila o modelo
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # Taxa de aprendizado um pouco menor
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

"""###4. Treinamento do Modelo

"""

EPOCHS = 100 # Aumentei o número máximo de épocas, pois o EarlyStopping vai parar antes se necessário

# Callback EarlyStopping: Para o treinamento quando a métrica de validação não melhora.
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=25, # Um pouco mais de paciência para datasets maiores
    restore_best_weights=True
)

# Callback ReduceLROnPlateau: Reduz a taxa de aprendizado quando a métrica de validação estagna.
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2, # Reduz a LR em 80%
    patience=12, # Espera um pouco mais antes de reduzir
    min_lr=0.000005, # LR mínima
    verbose=1
)

print("\nIniciando treinamento do modelo...")
history = model.fit(
    train_ds_vectorized,
    epochs=EPOCHS,
    validation_data=val_ds_vectorized, # Corrected variable name
    callbacks=[early_stopping, reduce_lr]
)
print("Treinamento concluído.")

# Visualizar o histórico de treinamento
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Acurácia de Treino')
plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')
plt.axvline(x=np.argmin(history.history['val_loss']) + 1, color='r', linestyle='--', label='Melhor Época (Val. Loss)')
plt.legend()
plt.title('Acurácia ao Longo das Épocas')
plt.xlabel('Época')
plt.ylabel('Acurácia')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Perda de Treino')
plt.plot(history.history['val_loss'], label='Perda de Validação')
plt.axvline(x=np.argmin(history.history['val_loss']) + 1, color='r', linestyle='--', label='Melhor Época (Val. Loss)')
plt.legend()
plt.title('Perda ao Longo das Épocas')
plt.xlabel('Época')
plt.ylabel('Perda')
plt.grid(True)

plt.tight_layout()
plt.show()

"""###5. Avaliação Final do Modelo no Conjunto de Teste

"""

print("\n--- Avaliação do Modelo no Conjunto de Teste ---")
loss, accuracy = model.evaluate(test_ds_vectorized)

print(f"Perda no conjunto de teste: {loss:.4f}")
print(f"Acurácia no conjunto de teste: {accuracy:.4f}")

# Para gerar um relatório de classificação mais detalhado
y_pred_probs = model.predict(test_ds_vectorized)
y_pred = (y_pred_probs > 0.5).astype("int32")

# Coletar os rótulos verdadeiros do conjunto de teste (necessário iterar o test_ds original para obter os labels)
# O test_ds_vectorized só tem os batches vetorizados, precisamos dos labels originais
y_true_test_labels = []
for _, label_batch in test_ds.as_numpy_iterator():
    y_true_test_labels.extend(label_batch)
y_true_test_labels = np.array(y_true_test_labels)


print("\nRelatório de Classificação no conjunto de teste:")
print(classification_report(y_true_test_labels, y_pred, target_names=['Falsa', 'Real']))

# Matriz de Confusão para uma visualização clara
conf_matrix = confusion_matrix(y_true_test_labels, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Falsa (Prevista)', 'Real (Prevista)'],
            yticklabels=['Falsa (Verdadeira)', 'Real (Verdadeira)'])
plt.title('Matriz de Confusão')
plt.xlabel('Previsto')
plt.ylabel('Verdadeiro')
plt.show()

"""###6. Exemplo de Previsão em Novas Notícias"""

print("\n--- Exemplo de Previsão em Novas Notícias ---")
new_news_articles = [
    # Notícias Falsas
    "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead",
    "House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the as.",
    "On Friday, it was revealed that former Milwaukee Sheriff David Clarke, who was being considered for ",
    # Notícias Reais
    "WASHINGTON (Reuters) - Alabama Secretary of State John Merrill said he will certify Democratic Senat",
    "WASHINGTON (Reuters) - Transgender people will be allowed for the first time to enlist in the U.S. m",
    "WASHINGTON (Reuters) - The special counsel investigation of links between Russia and President Trump"
]

# Vetoriza as novas frases usando a mesma camada que foi adaptada
vectorized_new_articles = text_vectorizer(tf.constant(new_news_articles))

# Faz a previsão com o modelo
predictions = model.predict(vectorized_new_articles)

# Exibe as previsões
for i, article_text in enumerate(new_news_articles):
    probability_real = predictions[i][0]
    predicted_class = "REAL" if probability_real > 0.5 else "FALSA"
    print(f"Notícia: '{article_text[:100]}...'") # Exibe apenas o começo
    print(f"Probabilidade REAL: {probability_real:.4f} -> Previsão: {predicted_class}\n")

# --- 5. SALVANDO O MODELO COMPLETO PARA DEPLOYMENT (NOVO CÓDIGO) ---

print("\n--- Salvando o modelo para deployment no Streamlit ---")

# Criar um novo modelo Sequential que inclui a camada de vetorização
# Isso cria um pipeline completo de texto bruto -> previsão
deploy_model = tf.keras.Sequential([
  text_vectorizer,  # Primeira camada: converte texto em vetores
  model             # Segunda camada: seu modelo treinado que recebe os vetores
])

# O novo modelo agora tem 2 camadas, mas a lógica de previsão é a mesma.
# Vamos verificar a previsão com ele para garantir que está funcionando.
print("\nTestando o modelo de deployment com as mesmas notícias de exemplo:")
# Assumindo que 'new_news_articles' está definido em uma seção anterior para teste
# Se não estiver, você pode definir uma pequena lista aqui para o teste de salvamento
# new_news_articles = ["This is a test article.", "Another test phrase."]
predictions_deploy = deploy_model.predict(tf.constant(new_news_articles))

# Exibe as previsões para confirmar
# Adapte conforme as classes do seu modelo (Falsa/Real)
for i, article_text in enumerate(new_news_articles):
    probability_real = predictions_deploy[i][0]
    predicted_class = "REAL" if probability_real > 0.5 else "FALSA"
    print(f"Notícia: '{article_text[:60]}...' -> Previsão do deploy_model: {predicted_class}")


# Salvar o modelo completo em um único arquivo no formato Keras v3
# Este é o único arquivo que você precisará para a sua aplicação Streamlit.
deploy_model.save("fake_news_classifier.keras") # <--- Este é o arquivo principal a ser usado no Streamlit

print("\nModelo completo salvo com sucesso como 'fake_news_classifier.keras'")
print("Faça o download deste arquivo do seu ambiente Colab para usá-lo com o Streamlit.")

# --- As linhas abaixo não são mais estritamente necessárias se você usar o .keras completo ---
# --- Mantenha-as COMENTADAS ou REMOVIDAS para evitar confusão de arquivos ---
# model_save_path = 'fake_news_detector_model.h5'
# vectorizer_save_path = 'text_vectorizer_config.json'
# model.save(model_save_path) # Você pode remover esta linha
# print(f"Modelo salvo em: {model_save_path}")
# import json # Pode remover se não for usar mais
# vectorizer_config = {'config': text_vectorizer.get_config(), 'weights': text_vectorizer.get_weights()}
# with open(vectorizer_save_path, 'w') as f:
#     json.dump(vectorizer_config, f) # Você pode remover esta linha
# print(f"TextVectorizer salvo em: {vectorizer_save_path}")
# print("\nModelo e TextVectorizer salvos com sucesso!")

# --- 7. SALVANDO O MODELO COMPLETO PARA DEPLOYMENT ---

print("\n--- Salvando o modelo para deployment no Streamlit ---")

# Criar um novo modelo Sequential que inclui a camada de vetorização
# Isso cria um pipeline completo de texto bruto -> previsão
deploy_model = tf.keras.Sequential([
  text_vectorizer,  # Primeira camada: converte texto em vetores
  model             # Segunda camada: seu modelo treinado que recebe os vetores
])

# O novo modelo agora tem 2 camadas, mas a lógica de previsão é a mesma.
# Vamos verificar a previsão com ele para garantir que está funcionando.
print("\nTestando o modelo de deployment com as mesmas notícias de exemplo:")
predictions_deploy = deploy_model.predict(tf.constant(new_news_articles))

# Exibe as previsões para confirmar
for i, article_text in enumerate(new_news_articles):
    probability_real = predictions_deploy[i][0]
    predicted_class = "REAL" if probability_real > 0.5 else "FALSA"
    print(f"Notícia: '{article_text[:60]}...' -> Previsão do deploy_model: {predicted_class}")


# Salvar o modelo completo em um único arquivo no formato Keras v3
# Este é o único arquivo que você precisará para a sua aplicação Streamlit.
deploy_model.save("fake_news_classifier.keras")

print("\nModelo completo salvo com sucesso como 'fake_news_classifier.keras'")
print("Faça o download deste arquivo do seu ambiente Colab para usá-lo com o Streamlit.")

